!echo "ttf-mscorefonts-installer msttcorefonts-eula select true" | sudo debconf-set-selections
!sudo apt-get install msttcorefonts ttf-mscorefonts-installer -y --reinstall
!sudo fc-cache -f
!fc-match "Arial"

!pip install seaborn[stats]
!pip install scipy
!pip install numpy
!pip install matplotlib
!pip install pandas
!pip install sklearn
!pip install imblearn
!pip install joblib

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as mpl
import scipy.stats as stats
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, ShuffleSplit,cross_validate
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.decomposition import PCA
from imblearn.over_sampling import SVMSMOTE
from joblib import dump, load

font_paths = mpl.font_manager.findSystemFonts()
for font_file in font_paths:
  mpl.font_manager.fontManager.addfont(font_file)

plt.rcParams['font.family'] = 'Arial'
plt.rcParams['font.sans-serif'] = 'Arial'

sns.set_theme(style="white", rc={"grid.linewidth": 0.1})
sns.set_style("ticks")
sns.set(font='Arial')
sns.set_context("paper", font_scale=0.9)
plt.figure(figsize=(3.1, 3))
sns.despine(left=False, bottom=False)



files_paths = ['./drive/MyDrive/dataset_map_tp1/turnover.csv','./drive/MyDrive/dataset_map_tp1/turnover_prepared_1.csv']
datasets = []
for path in files_paths:
  #get csv file encoding
  df = pd.read_csv(path, encoding='Latin-1')

  df.columns = df.columns.str.capitalize()
  datasets.append(df)
print(datasets[1].shape)
datasets[1].head()

#Create set of columns name per types
CATEGORY_COLUMNS= ['Industry', 'Profession','Traffic', 'Coach', 'Head_gender','Greywage','Way','Gender']
QUANTITATIVE_COLUMNS = ['Age','Stag','Novator','Anxiety','Selfcontrol','Independ','Extraversion','Wayencoded']
REAL_QUANTITATIVE_COLUMNS = ['Stag', 'Novator','Anxiety','Selfcontrol','Independ','Extraversion']
INTEGER_QUANTITATIVE_COLUMNS = ['Age']
EXPLICATIVES_COLUMNS = CATEGORY_COLUMNS + QUANTITATIVE_COLUMNS
#Target of classification variable
TARGET = 'Event'
COLOR_PALETTE = ['#8B383F','#373D61','#203AE0']

print(datasets[0].isnull().sum())


print(datasets[1].isnull().sum())

for column in CATEGORY_COLUMNS:
  print(f'Values of {column} are : ',','.join(datasets[0][column].unique()))


def get_description(dataset):
  description = dataset.describe(include='all')
  description = dataset.describe(include='all')
  description.loc['var'] = dataset.var().tolist()
  description.loc['skew'] = dataset.skew().tolist()
  description.loc['kurt'] = dataset.kurtosis().tolist()
  return description
print(get_description(datasets[1]))
stats.normaltest(datasets[1][TARGET])

data = datasets[1][TARGET].value_counts()

data.values


splot = plt.pie(data, labels=['Rotation', 'Pas de rotation'], colors=COLOR_PALETTE, autopct='%.0f%%')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()
# displaying chart
plt.savefig('pie_chart_rotation.png', bbox_inches='tight')


import matplotlib as mpl



for column in INTEGER_QUANTITATIVE_COLUMNS:
  splot = sns.displot(
      datasets[1],
      x=column, hue=TARGET,
      multiple="stack",
      palette=COLOR_PALETTE,
      edgecolor=".3",
      linewidth=.7,
      stat="probability",
      log_scale=False,
  )

  x_ticks = [int(x) for x in datasets[1][column].unique()]
  splot.ax.set_xticks( [min(x_ticks), int(datasets[1][column].mean()) ,max(x_ticks)])
  plt.axvline(datasets[1][column].median(), color=COLOR_PALETTE[2],linestyle='--', label='Médiane')
  splot.legend.remove()
  splot.ax.set_ylabel("Probabilité")
  splot.ax.set_xlabel("")
  plt.show()
  plt.savefig(f'distribution_{column}.png')

for column in REAL_QUANTITATIVE_COLUMNS:
  splot = sns.displot(data=datasets[1], x=column, kind="kde",palette=COLOR_PALETTE,  bw_adjust=.25)
  plt.legend().remove()
  x_ticks = [int(x) for x in datasets[1][column].unique()]
  splot.ax.set_xticks( [min(x_ticks), int(datasets[1][column].mean()) ,max(x_ticks)])
  plt.axvline(datasets[1][column].median(), color=COLOR_PALETTE[2],linestyle='--', label='Médiane')
  splot.ax.set_ylabel("Densité")
  splot.ax.set_xlabel("")
  plt.show()
  plt.savefig(f'distribution_{column}.png')

scaler = StandardScaler()
f_scaler = scaler
b = datasets[1].copy()
b[EXPLICATIVES_COLUMNS] = scaler.fit_transform(b[EXPLICATIVES_COLUMNS])

b[QUANTITATIVE_COLUMNS].boxplot()

splot = datasets[1][QUANTITATIVE_COLUMNS].boxplot()
splot.yaxis.grid(True, clip_on=False)
plt.xticks(rotation=90)
plt.tight_layout()
#sns.despine(left=True, bottom=True)
plt.show()
plt.savefig('main_boxplot.png')

IQR_dataset = b.copy()

# Columns to remove outlier
columns = ['Stag','Age']
def remove_outliers(IQR_dataset, columns):
  for column in columns:
    # IQR
    Q1 = np.percentile(IQR_dataset[column], 25, method='midpoint')
    Q3 = np.percentile(IQR_dataset[column], 75, method='midpoint')
    IQR = Q3 - Q1
    print(f"IQR of {column} ", IQR)

    # Above Upper bound
    upper = Q3+1.5*IQR
    upper_array = np.array(IQR_dataset[column] >= upper)
    print(f"Upper Bound of {column}:", upper)
    print(upper_array.sum())

    # Below Lower bound
    lower = Q1-1.5*IQR
    lower_array = np.array(IQR_dataset[column] <= lower)
    print(f"Lower Bound of {column}:", lower)
    print(lower_array.sum())


    # Create arrays of Boolean values indicating the outlier rows
    upper_array = np.where(IQR_dataset[column] >= upper)[0]
    lower_array = np.where(IQR_dataset[column] <= lower)[0]

    # Removing the outliers
    IQR_dataset.drop(index=upper_array, inplace=True)
    IQR_dataset.drop(index=lower_array, inplace=True)
    IQR_dataset.reset_index(drop=True, inplace=True)
    # Print the new shape of the DataFrame
    print(f"New Shape after removing outliers on column {column}: ", IQR_dataset.shape)
    IQR_dataset[columns].boxplot(figsize=(3.1, 3))
    plt.xticks(rotation=90)
    #plt.tight_layout()
    plt.show()
  return IQR_dataset
IQR_dataset = remove_outliers(IQR_dataset, columns)
#IQR_dataset = remove_outliers(IQR_dataset, columns)
#IQR_dataset = remove_outliers(IQR_dataset, columns)


IQR_dataset[QUANTITATIVE_COLUMNS].boxplot(figsize=(3.1, 3))
plt.xticks(rotation=90)
#plt.tight_layout()
plt.show()
plt.savefig('main_boxplot.png')

IQR_dataset.shape

plt.figure(figsize=(14, 6))
# define the mask to set the values in the upper triangle to True
mask = np.triu(np.ones_like(IQR_dataset.corr()))
heatmap = sns.heatmap(IQR_dataset.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')
plt.tight_layout()
plt.box(False)
plt.show()
plt.savefig('main_headmap.png')

ZScore_dataset = datasets[1].copy()






#z-score normalization
scaler = StandardScaler()
ZScore_dataset[EXPLICATIVES_COLUMNS] = scaler.fit_transform(ZScore_dataset[EXPLICATIVES_COLUMNS])
ZScore_dataset[QUANTITATIVE_COLUMNS].boxplot()

age_threshold_z = datasets[1]['Age'].std() * 2.8

outlier_indices = np.where(ZScore_dataset['Stag'] > 2.2)[0]
print(outlier_indices)
no_outliers = ZScore_dataset.drop(outlier_indices)
print("Original DataFrame Shape:", ZScore_dataset.shape)
print("DataFrame Shape after Removing Outliers:", no_outliers.shape)

no_outliers[QUANTITATIVE_COLUMNS].boxplot()
no_outliers.reset_index(inplace=True)

threshold_z = 2.8

outlier_indices = np.where(no_outliers['Age'] > threshold_z)[0]
print(outlier_indices, )
no_outliers_2 = no_outliers.drop(outlier_indices)
print("Original DataFrame Shape:", no_outliers.shape)
print("DataFrame Shape after Removing Outliers:", no_outliers_2.shape)

no_outliers_2[QUANTITATIVE_COLUMNS].boxplot()


no_outliers_2.shape

with sns.axes_style("white"):
  plt.figure(figsize=(16, 6))
  # define the mask to set the values in the upper triangle to True
  mask = np.triu(np.ones_like(no_outliers_2.corr()))
  heatmap = sns.heatmap(no_outliers_2.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')
  heatmap.bbox_inches = 'with'
  plt.tight_layout()
  sns.set_style("ticks")
  plt.box(False)
  plt.show()
  plt.savefig('out_main_headmap.png')

CATEGORY_COLUMNS= ['Industry', 'Profession','Traffic', 'Coach', 'Head_gender','Greywage','Way','Gender']
EXPLICATIVES_COLUMNS = CATEGORY_COLUMNS + QUANTITATIVE_COLUMNS
IQR_dataset = IQR_dataset[EXPLICATIVES_COLUMNS + [TARGET]]
no_outliers_2 = no_outliers_2[EXPLICATIVES_COLUMNS + [TARGET]]
print(IQR_dataset.shape)
print(no_outliers_2.shape)

ACP_dataset = IQR_dataset.copy()

def generate_components_short_names(n):
  return [f'C{i+1}' for i in range(n)]

# Créer un objet PCA
pca = PCA(random_state=0)

# Appliquer l'ACP aux données standardisées
pca.fit(ACP_dataset[EXPLICATIVES_COLUMNS])

# Composantes principales
components = pca.components_

# Variance expliquée
explained_variance = pca.explained_variance_ratio_

# Matrice des valeurs propres (Importance des facteurs)
eigenvalues = pca.explained_variance_

# Vecteurs propres (Construction des facteurs)
eigenvectors = pca.components_

# Contributions des variables aux composantes principales 1 et 2
contributions = pca.components_[:2, :].T

# Noms des variables
variable_names = ACP_dataset[EXPLICATIVES_COLUMNS].columns

# Matrice des valeurs propres
# Saturations (Importance des variables / facteurs)
loadings = eigenvectors * np.sqrt(eigenvalues.reshape(-1, 1))

# Créez un graphique de dispersion des contributions
plt.figure(figsize=(10, 6))
plt.scatter(contributions[:, 0], contributions[:, 1], marker='o', color='blue', alpha=0.5)
plt.xlabel('Contribution à CP1')
plt.ylabel('Contribution à CP2')

# Ajoutez les noms des variables
for i, variable in enumerate(variable_names):
    plt.annotate(variable, (contributions[i, 0], contributions[i, 1]))

plt.title('Contributions des Variables aux Composantes Principales 1 et 2')
plt.grid(color='white', linestyle='--', linewidth=0.5)
plt.box(False)
plt.axhline(0, color='black', linewidth=0.5)
plt.axvline(0, color='black', linewidth=0.5)
plt.show()

# Visualisation des valeurs propres
plt.figure(figsize=(8, 4))
plt.bar(range(1, len(eigenvalues) + 1), explained_variance *100, alpha=0.5, align='center')
plt.xlabel('Composante Principale')
plt.ylabel('Valeur Propre')
plt.title('Valeurs Propres')
plt.grid(color='gray', linestyle='--', linewidth=0.5)
plt.show()

plt.figure(figsize=(8, 4))
plt.bar(range(1, len(eigenvalues) + 1), eigenvalues, alpha=0.5, align='center')
plt.xlabel('Composante Principale')
plt.ylabel('Valeur Propre')
plt.title('Valeurs Propres')
plt.grid(color='gray', linestyle='--', linewidth=0.5)
plt.show()
pd.DataFrame(data = { 'TVE': eigenvalues, 'PVTE': explained_variance*100, 'CumPVTE': np.cumsum(explained_variance*100) },  index=generate_components_short_names(len(eigenvalues)))





from imblearn.over_sampling import SVMSMOTE


#nombre de composantes choisie
n = 7
pca = PCA(n_components=n, random_state=0)
sm = SVMSMOTE(random_state=0)

# Appliquer l'ACP aux données standardisées

reduced_ACP_dataset = pca.fit_transform(ACP_dataset[EXPLICATIVES_COLUMNS])
reduced_ACP_dataset = pd.DataFrame(data=reduced_ACP_dataset, columns=generate_components_short_names(n))
reduced_ACP_dataset[TARGET] = ACP_dataset[TARGET]

# Composantes principales
components = pca.components_

# Corretion du desequilibre
X, y = sm.fit_resample(reduced_ACP_dataset[generate_components_short_names(n)], reduced_ACP_dataset[TARGET])
print(X.shape)
# Diviser le dataset en ensembles d'entraînement et de test
cv = ShuffleSplit(n_splits=500, test_size=0.3, random_state=0)

# Créer un modèle de régression logistique avec régularisation L2 (Ridge)
alpha = 1  # Paramètre de régularisation, ajustez selon vos besoins
model = LogisticRegression( penalty='l2', random_state=0, C=1/alpha, max_iter=1000)

scoring = ['precision_micro', 'roc_auc','recall','accuracy','f1_micro']
# Entraîner le modèle sur les données d'entraînement
scores = cross_validate(model, X, y, cv=cv, scoring=scoring,return_train_score=True, return_estimator=True, return_indices=True)
print(scores.keys(),"%0.2f accuracy with a standard deviation of %0.2f" % (scores['test_accuracy'].mean(), scores['test_roc_auc'].std()))

# Selection du meilleur estimateur (test_recall, test_f1_micro, test_precision_micro, test_accuracy, test_roc_auc)
index = [np.where(scores['test_recall']==scores['test_recall'].max())[0],np.where(scores['test_f1_micro']==scores['test_f1_micro'].max())[0],np.where(scores['test_precision_micro']==scores['test_precision_micro'].max())[0], np.where(scores['test_accuracy']==scores['test_accuracy'].max())[0],np.where(scores['test_roc_auc']==scores['test_roc_auc'].max())[0]]
print(index)









Index = np.concatenate(index)
Metrics = ['test_recall', 'test_f1_micro', 'test_precision_micro', 'test_accuracy', 'test_roc_auc']
Performances = {'#Indices': []}
for i in range(len(Metrics)):
  Performances[Metrics[i]] = []
  for j in range(len(Index)):
    Performances[Metrics[i]].append(scores[Metrics[i]][Index[j]])
    Performances['#Indices'].append(Index[j])
es = pd.DataFrame(Performances,columns=Metrics, index=['E1','E2','E3','E4','E5'])
es.columns =  es.columns.str.capitalize()
es.columns =  es.columns.str.replace('_', ' ')
es['#Indice'] = Index
es


model = scores['estimator'][Index[0]]
X_test = X.iloc[scores['indices']['test'][Index[1]]]
y_test = y.iloc[scores['indices']['test'][Index[1]]]

# Faire des prédictions sur l'ensemble de test
y_pred = model.predict(X_test)

# Calculer les métriques de performance
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Afficher les métriques de performance
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

group_names = ['TN','FP','FN','VP']
group_counts = ["{0:0.0f}".format(value) for value in conf_matrix.flatten()]
group_percentages = ['{0:.2%}'.format(value) for value in conf_matrix.flatten()/np.sum(conf_matrix)]
labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(conf_matrix, annot=labels, fmt='', cmap='Blues')

# Sauvegarde des models
dump(model, 'model.joblib')
dump(f_scaler, 'scaler.joblib')
dump(pca, 'pca.joblib')
